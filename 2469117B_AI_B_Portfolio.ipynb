{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>2469117b Portfolio Notebook for AI for the Arts B</u>\n",
    "## <a href=\"https://github.com/AlexanderBroad/AI_B\">Github Repository</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u><b>First, please run the cell below to import the necessary dependencies for the Presentation:</b></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os, sys\n",
    "import shutil\n",
    "import glob\n",
    "import json\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import random\n",
    "from scipy.stats import entropy\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.utils import load_img, img_to_array\n",
    "import IPython.display as disp\n",
    "from IPython.display import HTML, Audio, Image, display_jpeg, display\n",
    "import librosa.display\n",
    "\n",
    "print(\"Thank you!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Context</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>The object, history, and objectives</u>\n",
    "\n",
    "The object chosen for my project is <b>MS Gen 13</b>, a late nineteenth century travel journal. The journal is located in the University of Glasgow's Special Collections. Full archival details are available <a href=\"https://www.gla.ac.uk/collections/#/details?irn=250262&catType=C&referrer=/results&q=ms+gen+13\">here.</a> Written by Glasgow lawyer Andrew MacGeorge, the journal chronicles a contintental tour he and his family undertook during the winter of 1867-1868, also containing some pen and ink illustrations and photographs of scenes from the holiday.\n",
    "<br><br>\n",
    "I chose this object as I felt that out of all the objects available to us, it had the most potential for Human-AI creative collaboration. The objective of the project is to engage a general audience with cultural heritage with the use of generative AI. Specifically, I was interested in using the sketches in the journal to create an engaging tool that could be implemented as an exhibit in a gallery/museum setting where users can read the journal and view MacGeorge's sketches, accompanied by AI-generated soundscapes relating to each sketch. Additionally, I wanted users to be able to upload their own images and have generative AI convert them into the style of MacGeorge's illustrations.\n",
    "<br><br>\n",
    "The objectives behind the project were:\n",
    "<ol> Allowing users to see their own images in the style of Macgeorge's sketches to give them a deeper appreciation of what it was like to travel long distances in the nineteenth century. </ol>\n",
    "<ol> Accompanying Macgeorge's sketches with soundscapes to:\n",
    "    <br> - increase their overall appeal and add value to the user experience.\n",
    "    <br> - broaden users' understanding of social and class differences in the past that meant holidaying abroad was only possible for the very wealthy, like Andrew Macgeorge and his family. </ol>\n",
    "<ol> Adding soundcapes to the images to increase inclusivity as they have the potential to engage partially-sighted users with the journal. </ol>\n",
    "<br>\n",
    "When undertaking initial research on the generative AI processes needed to meet my objectives, I made the decision to focus my efforts on the soundscape feature, as I felt the image-style generation element would increase the scope of the project too much for the timeframe of the course. Additionally I thought that in terms of cultural engagement the soundscape feature was more conducive to highlighting the social perspective the journal gives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Methodology</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Incorporating AI models</u>\n",
    "\n",
    "Initially my intention for implementing an AI-generated soundscape was to use two models in conjuction: a model for image classification to identify elements of an image; and a text-to-audio model to generate sounds, using the output of the first model as its input. In terms of Human-AI collaboration, human creativity is present in the form of MacGeorge's sketches, which provide the input for the image classification model, whereas the AI component is found in both the image classification model and in the text-to-audio model. Two issues arose when experimenting with this proposed workflow. The first was the innacuracy of image classification models when identifying objects in the sketches from the journal. The second issue was the viability of text-to-audio models for building a soundscape. I found that currently available text-to-audio models were unsuitable for producing long-length audio files needed for a soundscape, and generally performed better when generating short-length audio files.\n",
    "<br><br>\n",
    "To overcome the first challenge I used \"<b>Sketch to image</b>\", an app from <a href=\"https://openart.ai/home\">Open Art</a> to upscale the journal sketches to allow an image classification model to better identify the elements in them:\n",
    "![upscaling example](upscaling_example.jpg)\n",
    "<br><br>\n",
    "The second issue was overcome with the use of <a href=\"https://qosmo.jp/en/art/imaginarysoundscape\"><b>Imaginary Soundscape</b></a>, a project by <a href=\"https://qosmo.jp/en\">Qosmo AI Creativity & Music Lab</a>. This model \"imagines\" a soundscape from an image by matching sounds to that image. In this way, sounds are not generated by the model but rather selected from existing sound recordings. Specifically, two Convolutional Neural Networks (CNNs) are utilized. CNNs are a type of neural network often used in computer vision applications. They contain hidden layers that can recognise and differentiate features in an image, as well as determine the relative importance of features. One CNN (based on MIT's <a href=\"http://soundnet.csail.mit.edu/\">SoundNet</a>) is trained to extract audio features from a dataset of unlabeled video, creating a database of environmental sounds. Another CNN extracts visual features from an image, then matches them to the most appropriate sound from the database.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>Diagram of the Model from the Imaginary Soundscape paper:</u>\n",
    "![Model](https://github.com/Kajiyu/ImaginarySoundscapeDemo/raw/master/assets/model.jpg)\n",
    "Credit: \"Imaginary Soundscape: Cross-Modal Approach to Generate Pseudo Sound Environments\" by Yuma Kajihara, Shoya Dozono, Nao Tokui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this process, the interaction between AI and humans can be observed. Humans create the dataset of environmental sounds by recording sound and video of the real world, forming the training data for the Audio CNN. Additonally we supply the vast dataset of images upon which the Visual CNN trains, and we capture (or in this case draw) the image on which the soundscape is based. AI matches features extracted from these elements to select a sound that best matches what MacGeorge drew. Finally, humans engage with the journal by reading it, viewing the sketches and listening to their associated soundscapes. In this way, my approach meets the objectives stated earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Presentation</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following presentation is based on the Imaginary Soundscape demo code, available at: https://github.com/QosmoInc/ImaginarySoundscapeDemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Before you try...</u>\n",
    "Please download this <a href=\"https://github.com/Kajiyu/ImaginarySoundscapeDemo/releases/download/v1.0.0/data.zip\">zipped data file</a> and unzip it into the same directory as this notebook. Thank you!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>1. Please run the following six cells to define the functions needed for preprocessing of images, feature extraction, selecting sounds, and creating an image grid.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines a function that normalises, shifts, and scales up the values of pixels in an image.\n",
    "# This is all the image preprocessing needed for our purposes.\n",
    "def preprocess_image(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines a function that extracts features from an image using a pre-trained model, ImageNet.\n",
    "def get_image_features(model, filepath):\n",
    "    img = load_img(filepath, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_image(x)\n",
    "    preds = model.predict(x)[0]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines a function that loads an audio file, scales it up and normalises it, then extracts features from it using the pre-trained SoundNet model.\n",
    "def get_sound_features(model,filepath):\n",
    "    x, sr = librosa.load(filepath)\n",
    "    x = x * 255.0\n",
    "    x = np.reshape(x, (1, x.shape[0], 1, 1))\n",
    "    x[x < -255.] = -255.\n",
    "    x[x > 255.] = 255.\n",
    "    assert np.max(x) < 256., \"It seems this audio contains signal that exceeds 256\"\n",
    "    assert np.min(x) > -256., \"It seems this audio contains signal that exceeds -256 \" + str(np.min(x)) + \" : \" + filepath\n",
    "    _y_pred = model.predict(x)\n",
    "    feature =  _y_pred[0][0][0][0]\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines a function that matches the features extracted from the image and sound files to give the top 'k' songs most similar to the image, where 'k' is an integer specifying the number of songs to return. \n",
    "def get_topk_songs(img_feature, song_features, k=3):\n",
    "    distances = []\n",
    "    for i in range(len(song_features)):\n",
    "        distance = entropy(img_feature, song_features[i])\n",
    "        distances.append(distance)\n",
    "    topk_indecies = np.argsort(distances)[:k]\n",
    "    return topk_indecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines a function with two outputs: a JSON file containing information about sound files, and a numpy file containing features extracted from the sound files.\n",
    "def create_sound_libs(\n",
    "    soundnet_model,\n",
    "    query=\"./data/sound_samples/*.mp3\",\n",
    "    out_json_path=\"./data/sounds.json\",\n",
    "    out_npy_path='./data/song_features.npy'\n",
    "):\n",
    "    mp3_files = glob.glob(query)\n",
    "    output_dict = {\"mp3\":[]}\n",
    "    for i in range(len(mp3_files)):\n",
    "        x, sr = librosa.load(mp3_files[i])\n",
    "        if len(x) > 230000:\n",
    "            output_dict[\"mp3\"].append({\"filepath\":mp3_files[i], \"index\":str(i)})\n",
    "    with open(out_json_path, \"w\")  as f:\n",
    "        json.dump(output_dict, f, ensure_ascii=False, indent=4, sort_keys=True, separators=(',', ': '))\n",
    "    song_features = []\n",
    "    for item in output_dict[\"mp3\"]:\n",
    "        i_feature = get_sound_features(soundnet_model,item['filepath'])\n",
    "        song_features.append(i_feature)\n",
    "    song_features = np.array(song_features)\n",
    "    print(\"Sound npy shape:\", song_features.shape)\n",
    "    np.save(out_npy_path, song_features)\n",
    "    return (output_dict, song_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates a grid with HTML displaying each image and their filename.\n",
    "photo_dir = './images'\n",
    "\n",
    "# Gets a list of image files in the directory.\n",
    "image_files = [file for file in os.listdir(photo_dir) if file.endswith(('jpg', 'png', 'jpeg'))]\n",
    "\n",
    "# Generates HTML for the grid.\n",
    "image_grid = '<div style=\"display: flex; flex-wrap: wrap;\">'\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(photo_dir, image_file)\n",
    "    image_grid += f'<div style=\"flex: 20%; padding: 5px; text-align: center;\">'\n",
    "    image_grid += f'<img src=\"{image_path}\" style=\"width: 120%;\"><br>{image_file}'\n",
    "    image_grid += '</div>'\n",
    "image_grid += '</div>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <u>Load SoundNet and ImageNet models by running the following cell:</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code loads the pre-trained models soundnet (for audio feature extraction) and MobileNet (for image classification).\n",
    "soundnet = load_model(\"./data/models/soundnet.hdf5\")\n",
    "imagenet = MobileNet(include_top=True, weights='imagenet')\n",
    "# NOTE: It was unclear why the image classification model places365_squeezenet is included in the zipped data file but not used here in place of MobileNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>3. Create sound features using SoundNet model by running the cell below (please be patient, it takes a while):</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code creates sound features using the SoundNet model.\n",
    "(mp3_files, sound_features) = create_sound_libs(soundnet)\n",
    "\n",
    "print(\"#of mp3 files\", len(mp3_files['mp3']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, this code loads the pre-processed JSON data.\n",
    "with open(\"./data/sounds.json\") as f:\n",
    "    mp3_files = json.load(f)\n",
    "sound_features = np.load(\"./data/song_features.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>4. Run the cell below to see the journal's sketches and photos, including some that I upscaled using \"Sketch to image\":</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the image grid.\n",
    "HTML(image_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>5. Now load the image you wish the model to use by replacing the filename in</u> IMG_PATH <u>with one of the above. Then run the cell:</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = \"./images/young girl.jpg\" #Replace with the filename of your choice after \"./images/...\"\n",
    "display_jpeg(Image(IMG_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>6. Run the following cell to have the model extract features from the image and match them to the best sound files.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts image features using ImageNet model(MobileNet).\n",
    "img_feature = get_image_features(imagenet, IMG_PATH)\n",
    "\n",
    "# Matches the image features and pre-processed sound features, then finds the top 'K' best matched sound files.\n",
    "topk_indecies = get_topk_songs(img_feature, sound_features, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>7. Run the cell below to listen to the top 3 sounds the model associated with the image!</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displays the top 3 best matched sounds.\n",
    "print(\"The best-matched sounds:\")\n",
    "print\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Top #\", i+1)\n",
    "    display(Audio(mp3_files[\"mp3\"][topk_indecies[i]][\"filepath\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Conclusions</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hoped that this presentation gave a sufficient guide of how I would incorporate generative AI to enhance user engagement with the journal. I envisage this project being implemented in either a museum exhibit or online portal where users can read a digitised verson of the journal (which has already been by <a href=\"./MSGen13-2.html\">transcribed</a> by Robert Maclean of the University of Glasgow's Special Collections) while listening to the soundscapes generated from MacGeorges sketches and photographs. The soundscape feature adds value to the viewing experience of the sketches and by extension the journal itself, thus meeting this objective set out at the beginning of the notebook. It is difficult to state whether the project has broadened users' understanding of 19th century class differences relating to holidaying, as the project is not available to the public, but it is my belief this objective would be met if the project was made publicly available in the manner described earlier.\n",
    "<br><br>\n",
    "There are still issues that remain pertaining to the model's accuracy. Even when using the upscaled versions of the sketches the model occasionally \"sees\" objects that are not there and consequently makes errors in matching relevant sounds to the images. I have found the model performs best when used with the photographs present in the journal, which demonstrates that the visual CNN training data mostly comes from real life photography. Interestingly, the model often produces recordings of musical performances when given the original sketches to analyse. This could be due to MobileNet misidentifying ink and pen illustrations as handwritten music manuscripts.\n",
    "<br><br>\n",
    "In terms of future development and potential improvements, obviously it would be prudent to implement the most cutting edge image classifaction models to increase accuracy. In terms of audio selection a highly tailored dataset could aid in auditory accuracy, since many of the audio samples utilized by the SoundNet model contain modern-day sounds that should not be present in a soundscape depicting events in the late 19th century. Additionally, as the third objective for this project was to increase the journal's appeal for visually impaired users, a future development could involve the implementation of a text-to-audio feature so users could listen to the text and generated soundscapes simultaneously. In this way I  believe the project in its current form only partially meets the final objective of increasing inclusivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- MS Gen 13 Collections page: https://www.gla.ac.uk/collections/#/details?irn=250262&catType=C&referrer=/results&q=ms+gen+13\n",
    "- Open Art: https://openart.ai/home\n",
    "- Imaginary Soundscape: https://qosmo.jp/en/art/imaginarysoundscape (Unfortunately the original website demo is not currently available)\n",
    "- Imaginary Soundscape demo code: https://github.com/QosmoInc/ImaginarySoundscapeDemo (Licenced under the MIT Licence, Copyright 2018 Yuma Kajihara)\n",
    "- Imaginary Soundcape model is based on MIT's SoundNet: http://soundnet.csail.mit.edu/\n",
    "- Imaginary Soundscape paper: https://nips2017creativity.github.io/doc/Imaginary_Soundscape.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
